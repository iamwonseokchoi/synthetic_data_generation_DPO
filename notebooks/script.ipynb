{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Synthetic Dataset Generation for DPO\n",
    "---\n",
    "Aug. 5, 2024"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Topic and Subtopic Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import vertexai\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "def init_vertexai(PROJECT_ID: str, REGION: str):\n",
    "    vertexai.init(project=PROJECT_ID, location=REGION)\n",
    "\n",
    "def create_llm(api_key: str, temperature: float = 0.8, top_p: float = 0.95):\n",
    "    return ChatOpenAI(\n",
    "        base_url=f\"https://{REGION}-aiplatform.googleapis.com/v1beta1/projects/{PROJECT_ID}/locations/{REGION}/endpoints/openapi/chat/completions?\",\n",
    "        model=\"meta/llama3-405b-instruct-maas\",\n",
    "        api_key=api_key,\n",
    "        max_tokens=4096,\n",
    "        temperature=temperature,\n",
    "        top_p=top_p,\n",
    "    )\n",
    "\n",
    "def create_prompt_template():\n",
    "    return ChatPromptTemplate.from_messages([\n",
    "        (\"system\", \"<|begin_of_text|><|start_header_id|>system<|end_header_id|>{system}<|eot_id|>\"),\n",
    "        (\"human\", \"<|start_header_id|>user<|end_header_id|>{human}<|eot_id|>\"),\n",
    "        (\"ai\", \"<|start_header_id|>assistant<|end_header_id|>{ai}<|eot_id|>\"),\n",
    "    ])\n",
    "\n",
    "def execute_llama_chain(system: str, user: str, llm: ChatOpenAI, prompt: ChatPromptTemplate, stream: bool = True):\n",
    "    chain = prompt | llm | StrOutputParser()\n",
    "    messages = [\n",
    "        SystemMessage(content=system),\n",
    "        HumanMessage(content=user),\n",
    "    ]\n",
    "    if stream:\n",
    "        return chain.stream({\"system\": system, \"human\": user, \"ai\": \"\"})\n",
    "    else:\n",
    "        return chain.invoke({\"system\": system, \"human\": user, \"ai\": \"\"})\n",
    "\n",
    "# Initialize Vertex AI and create LLM\n",
    "init_vertexai(PROJECT_ID, REGION)\n",
    "llm = create_llm(get_credentials())\n",
    "prompt = create_prompt_template()\n",
    "\n",
    "# Generate topics and subtopics\n",
    "topics_system = \"You are a master categorizer.\"\n",
    "topics_user = \"\"\"\n",
    "Generate a list of 10 diverse topics related to current events, technology, and general knowledge.\n",
    "Only generate the number of topics requested.\n",
    "The topics should be separated by a comma. There must be no other text than the list.\n",
    "\"\"\"\n",
    "\n",
    "full_topics = \"\"\n",
    "for chunk in execute_llama_chain(system=topics_system, user=topics_user, llm=llm, prompt=prompt):\n",
    "    full_topics += chunk\n",
    "\n",
    "topics = [topic.strip() for topic in full_topics.split(\",\")]\n",
    "\n",
    "for topic in topics:\n",
    "    subtopics_user = f\"\"\"\n",
    "    Given the topic \"{topic}\", generate a list of 5 subtopics that are related to it.\n",
    "    Only generate the number of subtopics requested.\n",
    "    The subtopics should be separated by a comma. There must be no other text than the list.\n",
    "    \"\"\"\n",
    "    \n",
    "    full_subtopics = \"\"\n",
    "    for chunk in execute_llama_chain(system=topics_system, user=subtopics_user, llm=llm, prompt=prompt):\n",
    "        full_subtopics += chunk\n",
    "    \n",
    "    subtopics = [subtopic.strip() for subtopic in full_subtopics.split(\",\")]\n",
    "    print(f\"Topic: {topic}\")\n",
    "    print(f\"Subtopics: {subtopics}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Question Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_questions(topic: str, subtopic: str, n_questions: int = 5):\n",
    "    questions_system = f\"\"\"\n",
    "    You are a guru in the topic of {topic} with a particular specialization in {subtopic}.\n",
    "    You are giving a quiz to your students of varying levels of expertise and age.\n",
    "    \"\"\"\n",
    "    \n",
    "    questions_user = f\"\"\"\n",
    "    Generate exactly {n_questions} questions that could be asked about {subtopic}.\n",
    "    The questions should be of varying difficulty and complexity.\n",
    "    Your response should be in a list format.\n",
    "    The questions should be separated by \"|\". There must be no other text than the list.\n",
    "    \"\"\"\n",
    "    \n",
    "    full_questions = \"\"\n",
    "    for chunk in execute_llama_chain(system=questions_system, user=questions_user, llm=llm, prompt=prompt):\n",
    "        full_questions += chunk\n",
    "    \n",
    "    return [question.strip() for question in full_questions.split(\"|\")]\n",
    "\n",
    "questions_list = []\n",
    "for topic in topics:\n",
    "    for subtopic in subtopics:\n",
    "        questions = generate_questions(topic, subtopic)\n",
    "        questions_list.extend([{\"topic\": topic, \"subtopic\": subtopic, \"question\": q} for q in questions])\n",
    "\n",
    "print(f\"Generated {len(questions_list)} questions.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Answer Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A. Llama-3.1-405B-Instruct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_answers(question: str, topic: str, subtopic: str):\n",
    "    answer_system = f\"\"\"\n",
    "    You are a master of the topic {topic} with a specialization and PhD in {subtopic}.\n",
    "    \"\"\"\n",
    "    \n",
    "    answer_user = f\"\"\"\n",
    "    Given the question: \"{question}\"\n",
    "    Generate 2 responses (response A and response B) to the question.\n",
    "    Insert response A below where it says RESPONSE A: and insert response B below where it says RESPONSE B:\n",
    "    RESPONSE A:\n",
    "    RESPONSE B:\n",
    "    \n",
    "    The above is the output format and should not be changed. Do not output anything else other than the responses.\n",
    "    \"\"\"\n",
    "    \n",
    "    full_answers = \"\"\n",
    "    for chunk in execute_llama_chain(system=answer_system, user=answer_user, llm=llm, prompt=prompt):\n",
    "        full_answers += chunk\n",
    "    \n",
    "    parts = full_answers.split(\"RESPONSE B:\")\n",
    "    response_a = parts[0].replace(\"RESPONSE A:\", \"\").strip()\n",
    "    response_b = parts[1].strip()\n",
    "    \n",
    "    return {\"response_a\": response_a, \"response_b\": response_b}\n",
    "\n",
    "answers_list = []\n",
    "for item in questions_list:\n",
    "    answers = generate_answers(item[\"question\"], item[\"topic\"], item[\"subtopic\"])\n",
    "    answers_list.append({**item, \"responses\": answers})\n",
    "\n",
    "print(f\"Generated answers for {len(answers_list)} questions.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B. Gemini-1.5-pro-experimental (0801)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import google.generativeai as genai\n",
    "\n",
    "# Configure the Gemini API\n",
    "genai.configure(api_key=\"YOUR_GEMINI_API_KEY\")\n",
    "\n",
    "def generate_answers_gemini(question: str, topic: str, subtopic: str):\n",
    "    model = genai.GenerativeModel('gemini-1.5-pro')\n",
    "    \n",
    "    prompt = f\"\"\"\n",
    "    As an expert in {topic} with a specialization in {subtopic}, provide two distinct responses to the following question:\n",
    "\n",
    "    Question: {question}\n",
    "\n",
    "    RESPONSE A:\n",
    "    [Your first response here]\n",
    "\n",
    "    RESPONSE B:\n",
    "    [Your second response here]\n",
    "\n",
    "    Ensure that both responses are informative and potentially correct, but with different approaches or perspectives.\n",
    "    \"\"\"\n",
    "    \n",
    "    response = model.generate_content(prompt)\n",
    "    \n",
    "    # Parse the response to extract Response A and Response B\n",
    "    content = response.text\n",
    "    parts = content.split(\"RESPONSE B:\")\n",
    "    response_a = parts[0].replace(\"RESPONSE A:\", \"\").strip()\n",
    "    response_b = parts[1].strip()\n",
    "    \n",
    "    return {\"response_a\": response_a, \"response_b\": response_b}\n",
    "\n",
    "# To use Gemini instead of Llama, replace the generate_answers function call in the previous loop\n",
    "answers_list = []\n",
    "for item in questions_list:\n",
    "    answers = generate_answers_gemini(item[\"question\"], item[\"topic\"], item[\"subtopic\"])\n",
    "    answers_list.append({**item, \"responses\": answers})\n",
    "\n",
    "print(f\"Generated answers using Gemini for {len(answers_list)} questions.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Evaluation and Scoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "def get_scores_from_response(openai_response):\n",
    "    logprobs = openai_response.choices[0].logprobs.content\n",
    "    score_dict = {}\n",
    "    for score in logprobs:\n",
    "        score_dict[score.token] = score.logprob\n",
    "    return score_dict\n",
    "\n",
    "def evaluate_responses(question: str, response_a: str, response_b: str):\n",
    "    client = OpenAI(\n",
    "        base_url=\"https://integrate.api.nvidia.com/v1\",\n",
    "        api_key=\"YOUR_NVIDIA_API_KEY\"\n",
    "    )\n",
    "    \n",
    "    messages_a = [\n",
    "        {\"role\": \"user\", \"content\": question},\n",
    "        {\"role\": \"assistant\", \"content\": response_a}\n",
    "    ]\n",
    "    \n",
    "    messages_b = [\n",
    "        {\"role\": \"user\", \"content\": question},\n",
    "        {\"role\": \"assistant\", \"content\": response_b}\n",
    "    ]\n",
    "    \n",
    "    response_a_eval = client.chat.completions.create(\n",
    "        model=\"nvidia/nemotron-4-340b-reward\",\n",
    "        messages=messages_a,\n",
    "    )\n",
    "    \n",
    "    response_b_eval = client.chat.completions.create(\n",
    "        model=\"nvidia/nemotron-4-340b-reward\",\n",
    "        messages=messages_b,\n",
    "    )\n",
    "    \n",
    "    scores_a = get_scores_from_response(response_a_eval)\n",
    "    scores_b = get_scores_from_response(response_b_eval)\n",
    "    \n",
    "    return {\n",
    "        \"response_a_score\": sum(scores_a.values()),\n",
    "        \"response_b_score\": sum(scores_b.values())\n",
    "    }\n",
    "\n",
    "# Evaluate and score the responses\n",
    "for item in answers_list:\n",
    "    scores = evaluate_responses(item[\"question\"], item[\"responses\"][\"response_a\"], item[\"responses\"][\"response_b\"])\n",
    "    item[\"scores\"] = scores\n",
    "\n",
    "print(\"Completed evaluation and scoring of responses.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Final Dataset Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_dataset(answers_list, threshold=3.5):\n",
    "    filtered_data = []\n",
    "    for item in answers_list:\n",
    "        if item[\"scores\"][\"response_a_score\"] >= threshold or item[\"scores\"][\"response_b_score\"] >= threshold:\n",
    "            filtered_data.append({\n",
    "                \"question\": item[\"question\"],\n",
    "                \"chosen\": item[\"responses\"][\"response_a\"] if item[\"scores\"][\"response_a_score\"] >= item[\"scores\"][\"response_b_score\"] else item[\"responses\"][\"response_b\"],\n",
    "                \"rejected\": item[\"responses\"][\"response_b\"] if item[\"scores\"][\"response_a_score\"] >= item[\"scores\"][\"response_b_score\"] else item[\"responses\"][\"response_a\"]\n",
    "            })\n",
    "    return filtered_data\n",
    "\n",
    "final_dataset = filter_dataset(answers_list)\n",
    "print(f\"Final dataset contains {len(final_dataset)} high-quality examples.\")\n",
    "\n",
    "# Save the final dataset\n",
    "import json\n",
    "with open(\"dpo_synthetic_dataset.json\", \"w\") as f:\n",
    "    json.dump(final_dataset, f, indent=2)\n",
    "\n",
    "print(\"Saved final dataset to dpo_synthetic_dataset.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### E.O.D"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

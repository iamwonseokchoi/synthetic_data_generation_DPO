# Synthetic Dataset Generator for DPO Fine-Tuning

This script generates a synthetic dataset for Direct Preference Optimization (DPO) fine-tuning of language models. It uses state-of-the-art language models to create a diverse set of questions and answers, which are then evaluated and filtered to produce a high-quality dataset.

## Prerequisites

- Python 3.12 (but older should work up to 3.10)
- Access to the following APIs (But you can use any model):
  - Vertex AI (for Llama-3.1-405B)
  - Google AI (for Gemini 1.5 Pro)
  - NVIDIA API (for Nemotron-4-340B-Reward)

## Installation

1. Clone this repository

2. Install the required packages


3. Set up your API keys and project details (if using GCP like the example)

## Usage

The script will:
1. Generate topics and subtopics
2. Create questions for each subtopic
3. Generate answers using Gemini 1.5 Pro
4. Evaluate and score the responses using Nemotron-4-340B-Reward
5. Filter the dataset based on quality scores
6. Save the final dataset as `dpo_synthetic_dataset.json`

## Output

The script produces a JSON file named `dpo_synthetic_dataset.json` containing the filtered, high-quality question-answer pairs suitable for DPO fine-tuning.

## Customization

- Adjust the `threshold` parameter in the `filter_dataset` function to control the quality of the final dataset.
- Modify the number of topics, subtopics, or questions generated by changing the respective parameters in the script.

## License

This project is licensed under the MIT License - see the LICENSE file for details.

## Acknowledgments

- This script uses models from Meta AI, Google AI, and NVIDIA.
- "Direct Preference Optimization: Your Language Model is Secretly a Reward Model" by Rafailov et al.
- Credits to Mervin Praison. Check out his Youtube Channel: [https://www.youtube.com/@MervinPraison]
